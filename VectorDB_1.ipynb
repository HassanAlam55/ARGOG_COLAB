{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2xB8tzXXj32"
      },
      "source": [
        "# Creating Vector DB for ARGOG Paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jb6x5_fwZhgY",
        "outputId": "79d2d5e4-ed3d-4b08-bbd4-93a85b529fda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasets not found. Installing...\n",
            "pandas is already installed.\n",
            "llama-index not found. Installing...\n",
            "chromadb not found. Installing...\n",
            "openai is already installed.\n",
            "dotenv is already installed.\n",
            "llama-index-vector-stores-postgres not found. Installing...\n",
            "llama-index-vector-stores-chroma not found. Installing...\n",
            "nest_asyncio is already installed.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# install packages if they dont exist:\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Function to install a package if not already installed\n",
        "def install_package(package_name, import_name=None):\n",
        "    try:\n",
        "        if import_name is None:\n",
        "            import_name = package_name\n",
        "        # Use __import__ for top-level modules, import for submodules\n",
        "        if \".\" in import_name:\n",
        "            exec(f\"from {import_name.rsplit('.', 1)[0]} import {import_name.rsplit('.', 1)[1]}\")\n",
        "        else:\n",
        "            __import__(import_name)\n",
        "    except ImportError:\n",
        "        print(f\"{package_name} not found. Installing...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
        "    else:\n",
        "        print(f\"{package_name} is already installed.\")\n",
        "\n",
        "# List of packages to install with their import names if different\n",
        "packages = [\n",
        "    (\"datasets\", \"datasets\"),\n",
        "    (\"pandas\", \"pandas\"),\n",
        "    (\"llama-index\", \"llama_index\"),\n",
        "    (\"chromadb\", \"chromadb\"),\n",
        "    (\"openai\", \"openai\"),\n",
        "    (\"dotenv\", \"dotenv\"),\n",
        "    (\"llama-index-vector-stores-postgres\", \"llama_index.vector_stores\"), # Install the postgres vector store if you intend to use it\n",
        "    (\"llama-index-vector-stores-chroma\", \"llama-index-vector-stores-chroma\"),\n",
        "    (\"nest_asyncio\", \"nest_asyncio\"),\n",
        "    (\"tenacity\", \"tenacity\")\n",
        "\n",
        "]\n",
        "\n",
        "# Install each package\n",
        "for package_name, import_name in packages:\n",
        "    install_package(package_name, import_name)\n",
        "\n",
        "# Check for `utils` module, typically custom or local\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/ARGOG')\n",
        "try:\n",
        "    import utils\n",
        "except ImportError:\n",
        "    print(\"The 'utils' module is not found. Make sure it's available in your environment or install it manually.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GeeZunHHev5Q"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries for loading datasets, data manipulation, document processing, vector storage, and embeddings.\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "from llama_index.core import Document, StorageContext, VectorStoreIndex\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
        "import chromadb\n",
        "from llama_index.core.node_parser import TokenTextSplitter\n",
        "from utils import chunked_iterable, load_config\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "import openai\n",
        "import os\n",
        "from utils import *\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogkjsIGaaLQt",
        "outputId": "fe0beaa7-1ae6-4802-f49b-348d134198aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Directory exists: /content/drive/MyDrive/Colab Notebooks/ARGOG\n",
            "run on: 2024-12-18 01:23:24.513355\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Verify the directory exists\n",
        "argog_path = '/content/drive/MyDrive/Colab Notebooks/ARGOG'\n",
        "if os.path.exists(argog_path):\n",
        "    print(f\"Directory exists: {argog_path}\")\n",
        "else:\n",
        "    print(f\"Directory does not exist: {argog_path}\")\n",
        "\n",
        "# Path to the .env file\n",
        "env_path = '/content/drive/MyDrive/Colab Notebooks/ARGOG/.env'\n",
        "\n",
        "# Load the .env file\n",
        "load_dotenv(dotenv_path=env_path)\n",
        "\n",
        "# Hardcoded values for easy adjustment\n",
        "CHUNK_SIZE = 1000 #only for db upload\n",
        "# TOKEN_CHUNK_SIZE = 512\n",
        "TOKEN_CHUNK_SIZE = 1024\n",
        "CHUNK_OVERLAP = 50\n",
        "\n",
        "# Load the config file\n",
        "load_config()\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "# print(openai.api_key)\n",
        "hf_read_token = os.getenv(\"HF_READ_TOKEN\")\n",
        "from huggingface_hub import login\n",
        "login(hf_read_token)\n",
        "print (f'run on: {pd.Timestamp.now()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to redo this to read the csv and sthen store it locally and retrieve it."
      ],
      "metadata": {
        "id": "sk5wx8X45MTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# Define the file path\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/ARGOG/archiv_data.csv'\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the DataFrame\n",
        "# print(df.head())\n"
      ],
      "metadata": {
        "id": "y0GV6jgTDEYo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AQR29bCoQoau"
      },
      "outputs": [],
      "source": [
        "# Specify the titles of the required papers\n",
        "required_paper_titles = [\n",
        "    'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',\n",
        "    'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter',\n",
        "    'HellaSwag: Can a Machine Really Finish Your Sentence?',\n",
        "    'LLaMA: Open and Efficient Foundation Language Models',\n",
        "    'Measuring Massive Multitask Language Understanding',\n",
        "    'CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks',\n",
        "    'Task2Vec: Task Embedding for Meta-Learning',\n",
        "    'GLM-130B: An Open Bilingual Pre-trained Model',\n",
        "    'SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems',\n",
        "    \"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\",\n",
        "    \"PAL: Program-aided Language Models\",\n",
        "    \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\",\n",
        "    \"DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LHM_8rNjSCaW"
      },
      "outputs": [],
      "source": [
        "# Filter the DataFrame to include only the required papers\n",
        "required_papers = df[df['title'].isin(required_paper_titles)]\n",
        "\n",
        "# Exclude the already selected papers to avoid duplicates and randomly sample ~40-50 papers\n",
        "remaining_papers = df[~df['title'].isin(required_paper_titles)].sample(n=40, random_state=123)\n",
        "\n",
        "# Concatenate the two DataFrames\n",
        "final_df = pd.concat([required_papers, remaining_papers], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iZ8GALGKSFkC"
      },
      "outputs": [],
      "source": [
        "# Prepare document objects from the dataset for indexing\n",
        "documents = [Document(text=content) for content in df['content']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-oiMETCycju1"
      },
      "outputs": [],
      "source": [
        "# Setup the embedding model\n",
        "embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
        "\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8ipRr3vH8pn3"
      },
      "outputs": [],
      "source": [
        "# from VS code notebook\n",
        "# Classic vector DB\n",
        "# Initialize a text splitter with hardcoded values for chunking documents\n",
        "parser = TokenTextSplitter(chunk_size=TOKEN_CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "nodes = parser.get_nodes_from_documents(documents)"
      ]
    },
    {
      "source": [
        "# !pip install tenacity\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import time\n",
        "from tenacity import retry, wait_exponential, stop_after_attempt\n",
        "\n",
        "# ... (your existing imports and code) ...\n",
        "\n",
        "# **Define insert_batch_size and sleep_duration**\n",
        "insert_batch_size = 10  # Adjust the batch size as needed\n",
        "sleep_duration = 2  # Adjust the sleep duration as needed\n",
        "\n",
        "# Instead of getting the collection, create it if it doesn't exist:\n",
        "chroma_collection = chroma_client.get_or_create_collection(\"ai_arxiv_full\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# Create the index object here\n",
        "index = VectorStoreIndex(\n",
        "    [],  # Initialize with an empty list of nodes\n",
        "    storage_context=storage_context,\n",
        "    embed_model=embed_model,\n",
        ")\n",
        "\n",
        "# **Change 1: Apply retry decorator with exponential backoff**\n",
        "@retry(wait=wait_exponential(multiplier=1, min=4, max=60), stop=stop_after_attempt(6))\n",
        "async def embed_with_retry(nodes_batch):\n",
        "    # Now 'index' is accessible here\n",
        "    return index.insert_nodes(nodes_batch)\n",
        "\n",
        "# **Change 2: Insert nodes in batches with a delay**\n",
        "for i in range(0, len(nodes), insert_batch_size):\n",
        "    batch_nodes = nodes[i : i + insert_batch_size]\n",
        "    # **Change 3: Call the decorated function to insert nodes**\n",
        "    await embed_with_retry(batch_nodes)\n",
        "    # Wait for the specified duration before processing the next batch\n",
        "    time.sleep(sleep_duration)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "gxJ38E7RWc3e",
        "outputId": "1cd5949c-e01d-4211-b4da-0bbeb28c690e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (8.5.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:llama_index.embeddings.openai.utils:Retrying llama_index.embeddings.openai.base.OpenAIEmbedding._get_text_embeddings.<locals>._retryable_get_embeddings in 0.661019219379 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
            "WARNING:llama_index.embeddings.openai.utils:Retrying llama_index.embeddings.openai.base.OpenAIEmbedding._get_text_embeddings.<locals>._retryable_get_embeddings in 0.5143622199263967 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
            "WARNING:llama_index.embeddings.openai.utils:Retrying llama_index.embeddings.openai.base.OpenAIEmbedding._get_text_embeddings.<locals>._retryable_get_embeddings in 0.3127849468844789 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
            "WARNING:llama_index.embeddings.openai.utils:Retrying llama_index.embeddings.openai.base.OpenAIEmbedding._get_text_embeddings.<locals>._retryable_get_embeddings in 0.5144950672133638 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
            "WARNING:llama_index.embeddings.openai.utils:Retrying llama_index.embeddings.openai.base.OpenAIEmbedding._get_text_embeddings.<locals>._retryable_get_embeddings in 0.7583838152646785 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
            "WARNING:llama_index.embeddings.openai.utils:Retrying llama_index.embeddings.openai.base.OpenAIEmbedding._get_text_embeddings.<locals>._retryable_get_embeddings in 0.3564120004342185 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RetryError",
          "evalue": "RetryError[<Future at 0x784a5cf3b0d0 state=finished raised RateLimitError>]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-c60a98684db4>\u001b[0m in \u001b[0;36membed_with_retry\u001b[0;34m(nodes_batch)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Now 'index' is accessible here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/indices/vector_store/base.py\u001b[0m in \u001b[0;36minsert_nodes\u001b[0;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"insert_nodes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minsert_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_index_struct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_struct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/indices/vector_store/base.py\u001b[0m in \u001b[0;36m_insert\u001b[0;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;34m\"\"\"Insert a document.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_nodes_to_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_struct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minsert_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/indices/vector_store/base.py\u001b[0m in \u001b[0;36m_add_nodes_to_index\u001b[0;34m(self, index_struct, nodes, show_progress, **insert_kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnodes_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insert_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mnodes_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_node_with_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0mnew_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vector_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minsert_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/indices/vector_store/base.py\u001b[0m in \u001b[0;36m_get_node_with_embedding\u001b[0;34m(self, nodes, show_progress)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \"\"\"\n\u001b[0;32m--> 139\u001b[0;31m         id_to_embed_map = embed_nodes(\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embed_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/indices/utils.py\u001b[0m in \u001b[0;36membed_nodes\u001b[0;34m(nodes, embed_model, show_progress)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     new_embeddings = embed_model.get_text_embedding_batch(\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mtexts_to_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/base/embeddings/base.py\u001b[0m in \u001b[0;36mget_text_embedding_batch\u001b[0;34m(self, texts, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m                 ) as event:\n\u001b[0;32m--> 335\u001b[0;31m                     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_text_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m                     \u001b[0mresult_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/embeddings/openai/base.py\u001b[0m in \u001b[0;36m_get_text_embeddings\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_retryable_get_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0mwrapped_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mexc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    417\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/embeddings/openai/base.py\u001b[0m in \u001b[0;36m_retryable_get_embeddings\u001b[0;34m()\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_retryable_get_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             return get_embeddings(\n\u001b[0m\u001b[1;32m    459\u001b[0m                 \u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/embeddings/openai/base.py\u001b[0m in \u001b[0;36mget_embeddings\u001b[0;34m(client, list_of_text, engine, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist_of_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/embeddings.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0;34m\"/embeddings\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1279\u001b[0m         )\n\u001b[0;32m-> 1280\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    958\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1047\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1096\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1047\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1096\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1047\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1096\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1047\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1096\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1047\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1096\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1047\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1096\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1047\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1096\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1047\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1096\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1047\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1096\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1047\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1096\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRetryError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-c60a98684db4>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mbatch_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minsert_batch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# **Change 3: Call the decorated function to insert nodes**\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mawait\u001b[0m \u001b[0membed_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Wait for the specified duration before processing the next batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep_duration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\u001b[0m in \u001b[0;36masync_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0masync_wrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;31m# Preserve attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mexc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    417\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_check\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRetryError\u001b[0m: RetryError[<Future at 0x784a5cf3b0d0 state=finished raised RateLimitError>]"
          ]
        }
      ]
    },
    {
      "source": [
        "# # this code works. but has batch. Try without batchin nex section\n",
        "# import nest_asyncio\n",
        "# nest_asyncio.apply()\n",
        "\n",
        "# # Continue with your existing code\n",
        "# # Instead of getting the collection, create it if it doesn't exist:\n",
        "# chroma_collection = chroma_client.get_or_create_collection(\"ai_arxiv_full\")\n",
        "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# # Reduce the batch size for embedding to avoid rate limits\n",
        "# # You can experiment with different values until it works reliably\n",
        "# # insert_batch_size = 10 # Commenting this out. Adjust the value if needed.\n",
        "\n",
        "# index = VectorStoreIndex(\n",
        "#     nodes,\n",
        "#     storage_context=storage_context,\n",
        "#     embed_model=embed_model,\n",
        "#     # use_async=True, # Commenting out to disable asynchronous execution\n",
        "#     # insert_batch_size=insert_batch_size # Commenting this out. Adjust the value if needed.\n",
        "# )"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "pwyEyXM3OrLa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zIOtRhOAE0ZG"
      },
      "outputs": [],
      "source": [
        "# # this code works. but has batch. Try without batchin nex section\n",
        "# # not working now\n",
        "# import nest_asyncio\n",
        "# nest_asyncio.apply()\n",
        "\n",
        "# # Continue with your existing code\n",
        "# # Instead of getting the collection, create it if it doesn't exist:\n",
        "# chroma_collection = chroma_client.get_or_create_collection(\"ai_arxiv_full\")\n",
        "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# # Reduce the batch size for embedding to avoid rate limits\n",
        "# # You can experiment with different values until it works reliably\n",
        "# insert_batch_size = 10\n",
        "\n",
        "# index = VectorStoreIndex(\n",
        "#     nodes,\n",
        "#     storage_context=storage_context,\n",
        "#     embed_model=embed_model,\n",
        "#     use_async=True,\n",
        "#     insert_batch_size=insert_batch_size # Add this line to control the batch size\n",
        "# )"
      ]
    },
    {
      "source": [
        "# this code works. but has batch. Try without batchin nex section\n",
        "# not working now\n",
        "# import nest_asyncio\n",
        "# nest_asyncio.apply()\n",
        "# import time\n",
        "\n",
        "# # Continue with your existing code\n",
        "# # Instead of getting the collection, create it if it doesn't exist:\n",
        "# chroma_collection = chroma_client.get_or_create_collection(\"ai_arxiv_full\")\n",
        "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# # Reduce the batch size for embedding to avoid rate limits\n",
        "# # You can experiment with different values until it works reliably\n",
        "# insert_batch_size = 5  # Reduced batch size further\n",
        "\n",
        "# index = VectorStoreIndex(\n",
        "#     nodes,\n",
        "#     storage_context=storage_context,\n",
        "#     embed_model=embed_model,\n",
        "#     use_async=True,\n",
        "#     insert_batch_size=insert_batch_size # Add this line to control the batch size\n",
        "# )\n",
        "\n",
        "# # Add a delay between batches to avoid rate limits\n",
        "# for i in range(0, len(nodes), insert_batch_size):\n",
        "#     # Process a batch of nodes\n",
        "#     batch_nodes = nodes[i : i + insert_batch_size]\n",
        "\n",
        "#     # ... your existing code to process the batch ...\n",
        "\n",
        "#     # Wait for a short duration before processing the next batch\n",
        "#     time.sleep(2) # Adjust the sleep duration as needed"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "JU0j3r_ImKvk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "source": [
        "# # this code works. but has batch. Try without batchin nex section\n",
        "# # not working now\n",
        "# import nest_asyncio\n",
        "# nest_asyncio.apply()\n",
        "# import time\n",
        "\n",
        "# # Continue with your existing code\n",
        "# # Instead of getting the collection, create it if it doesn't exist:\n",
        "# chroma_collection = chroma_client.get_or_create_collection(\"ai_arxiv_full\")\n",
        "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# # Reduce the batch size for embedding to avoid rate limits\n",
        "# # You can experiment with different values until it works reliably\n",
        "# insert_batch_size = 5  # Reduced batch size further\n",
        "\n",
        "# # **Change 1: Increase sleep duration**\n",
        "# sleep_duration = 60  # Increase sleep duration to 1 minute or more\n",
        "\n",
        "# index = VectorStoreIndex(\n",
        "#     nodes,\n",
        "#     storage_context=storage_context,\n",
        "#     embed_model=embed_model,\n",
        "#     use_async=True,\n",
        "#     insert_batch_size=insert_batch_size # Add this line to control the batch size\n",
        "# )\n",
        "\n",
        "# # Add a delay between batches to avoid rate limits\n",
        "# for i in range(0, len(nodes), insert_batch_size):\n",
        "#     # Process a batch of nodes\n",
        "#     batch_nodes = nodes[i : i + insert_batch_size]\n",
        "\n",
        "#     # ... your existing code to process the batch ...\n",
        "\n",
        "#     # Wait for a short duration before processing the next batch\n",
        "#     time.sleep(sleep_duration) # **Change 2: Apply the increased sleep duration**"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "3_4xYw6_qDFi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "source": [
        "# # this code works. but has batch. Try without batchin nex section\n",
        "# # not working now\n",
        "# import nest_asyncio\n",
        "# nest_asyncio.apply()\n",
        "# import time\n",
        "\n",
        "# # Continue with your existing code\n",
        "# # Instead of getting the collection, create it if it doesn't exist:\n",
        "# chroma_collection = chroma_client.get_or_create_collection(\"ai_arxiv_full\")\n",
        "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# # Reduce the batch size for embedding to avoid rate limits\n",
        "# # You can experiment with different values until it works reliably\n",
        "# insert_batch_size = 5  # Reduced batch size further\n",
        "\n",
        "# # **Change 1: Increase sleep duration**\n",
        "# sleep_duration = 120  # Increase sleep duration to 2 minutes or more\n",
        "\n",
        "# index = VectorStoreIndex(\n",
        "#     nodes,\n",
        "#     storage_context=storage_context,\n",
        "#     embed_model=embed_model,\n",
        "#     use_async=True,\n",
        "#     insert_batch_size=insert_batch_size # Add this line to control the batch size\n",
        "# )\n",
        "\n",
        "# # **Change 2: Insert nodes in batches with a delay**\n",
        "# for i in range(0, len(nodes), insert_batch_size):\n",
        "#     batch_nodes = nodes[i : i + insert_batch_size]\n",
        "#     # Insert the batch of nodes into the index\n",
        "#     index.insert_nodes(batch_nodes)\n",
        "#     # Wait for the specified duration before processing the next batch\n",
        "#     time.sleep(sleep_duration)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "QTgqownjr4Bi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # if the above works\n",
        "# # Sentence window\n",
        "# node_parser_sentence_window = SentenceWindowNodeParser.from_defaults(\n",
        "#     window_size=3,\n",
        "#     window_metadata_key=\"window\",\n",
        "#     original_text_metadata_key=\"original_text\",\n",
        "# )\n",
        "# nodes_sentence_window = node_parser_sentence_window.get_nodes_from_documents(documents)\n",
        "# nodes_sentence_window_filtered = filter_large_nodes(nodes_sentence_window)\n",
        "\n",
        "# # chroma_collection_sentence_window = chroma_client.create_collection(\"ai_arxiv_sentence_window\")\n",
        "\n",
        "# chroma_collection_sentence_window = chroma_client.create_collection(\n",
        "#     \"ai_arxiv_sentence_window\", get_or_create=True\n",
        "# )\n",
        "\n",
        "# vector_store_sentence_window = ChromaVectorStore(chroma_collection=chroma_collection_sentence_window)\n",
        "\n",
        "# storage_context_sentence_window = StorageContext.from_defaults(vector_store=vector_store_sentence_window)"
      ],
      "metadata": {
        "id": "wPYIgmbONhg3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "p7aClpyRA2s_"
      },
      "outputs": [],
      "source": [
        "# # try this code wihtout batch and see what happens.\n",
        "# # does not work without batch\n",
        "# import nest_asyncio\n",
        "# nest_asyncio.apply()\n",
        "\n",
        "# # Continue with your existing code\n",
        "# # Instead of getting the collection, create it if it doesn't exist:\n",
        "# chroma_collection = chroma_client.get_or_create_collection(\"ai_arxiv_full\")\n",
        "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# index = VectorStoreIndex(\n",
        "#     nodes,\n",
        "#     storage_context=storage_context,\n",
        "#     embed_model=embed_model,\n",
        "#     use_async=True\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ePI4Lt_eAhDj"
      },
      "outputs": [],
      "source": [
        "# import nest_asyncio\n",
        "# nest_asyncio.apply()\n",
        "\n",
        "# # Continue with your existing code\n",
        "# chroma_collection = chroma_client.get_collection(\"ai_arxiv_full\")\n",
        "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# index = VectorStoreIndex(\n",
        "#     nodes,\n",
        "#     storage_context=storage_context,\n",
        "#     embed_model=embed_model,\n",
        "#     use_async=True\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "vVFKJhgbcqLZ"
      },
      "outputs": [],
      "source": [
        "# Classic vector DB\n",
        "# # Initialize a text splitter with hardcoded values for chunking documents\n",
        "# parser = TokenTextSplitter(chunk_size=TOKEN_CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "# nodes = parser.get_nodes_from_documents(documents)\n",
        "\n",
        "# chroma_collection = chroma_client.create_collection(\"ai_arxiv_full\")\n",
        "\n",
        "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# index = VectorStoreIndex(\n",
        "#     nodes, storage_context=storage_context,\n",
        "#     embed_model=embed_model,\n",
        "#     use_async=True\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3GICuK61AzAM"
      },
      "outputs": [],
      "source": [
        "# # !pip install nest_asyncio\n",
        "# import nest_asyncio\n",
        "# nest_asyncio.apply()\n",
        "\n",
        "# # Classic vector DB\n",
        "# # Initialize a text splitter with hardcoded values for chunking documents\n",
        "# parser = TokenTextSplitter(chunk_size=TOKEN_CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "# nodes = parser.get_nodes_from_documents(documents)\n",
        "\n",
        "# # Instead of creating a new collection, try to get the existing one.\n",
        "# # If it doesn't exist, it will be created.\n",
        "# chroma_collection = chroma_client.get_or_create_collection(\"ai_arxiv_full\")\n",
        "\n",
        "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# # Remove use_async=True to avoid nested event loop issues.\n",
        "# index = VectorStoreIndex(\n",
        "#     nodes, storage_context=storage_context,\n",
        "#     embed_model=embed_model\n",
        "#     # use_async=True  # Removed this line\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LBMP706Hlk_z"
      },
      "outputs": [],
      "source": [
        "# !pip install nest_asyncio\n",
        "# import nest_asyncio\n",
        "# nest_asyncio.apply()\n",
        "\n",
        "# # Classic vector DB\n",
        "# # Initialize a text splitter with hardcoded values for chunking documents\n",
        "# parser = TokenTextSplitter(chunk_size=TOKEN_CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "# nodes = parser.get_nodes_from_documents(documents)\n",
        "\n",
        "# chroma_collection = chroma_client.create_collection(\"ai_arxiv_full\")\n",
        "\n",
        "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# index = VectorStoreIndex(\n",
        "#     nodes, storage_context=storage_context,\n",
        "#     embed_model=embed_model,\n",
        "#     use_async=True # This line triggers the error because an event loop is already running in Jupyter\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mroWs-Odnm9M"
      },
      "outputs": [],
      "source": [
        "# !pip install nest_asyncio\n",
        "# import nest_asyncio\n",
        "# nest_asyncio.apply()\n",
        "\n",
        "# # Classic vector DB\n",
        "# # Initialize a text splitter with hardcoded values for chunking documents\n",
        "# parser = TokenTextSplitter(chunk_size=TOKEN_CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "# nodes = parser.get_nodes_from_documents(documents)\n",
        "\n",
        "# # Instead of creating a new collection, try to get the existing one.\n",
        "# # If it doesn't exist, it will be created.\n",
        "# chroma_collection = chroma_client.get_or_create_collection(\"ai_arxiv_full\")\n",
        "\n",
        "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# index = VectorStoreIndex(\n",
        "#     nodes, storage_context=storage_context,\n",
        "#     embed_model=embed_model,\n",
        "#     use_async=True # This line triggers the error because an event loop is already running in Jupyter\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "PThaz499ll0z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1vVTa51vLMQZqca5YBz1M7YqbLPW7hT9_",
      "authorship_tag": "ABX9TyPRbhIM/H6RV3vBMeruSpVW"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}